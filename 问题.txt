运行命令bash: streamlit run yourpath/ui.py --server.fileWatcherType none
已知fps保持在30左右，视频检测中处理后的视频与图片检测得到的结果比起来质量太低。
我现在的方法是：内部处理每一帧，保证检测完整，并且写入。在前端页面每5帧更新一次显示，减少系统开销。

逐帧读取检测，cap.read() 返回的fame还是图像数据，然后用out.write(frame)写入创建的一个视频对象
只要图片检测质量可以，按理来说效果不会太差。为什么效果会远不如预期
我的得出的结论是有可能每个井盖的出场时间太少了，在本身就很少的帧里有可能也只有一部分”离散帧“被检测出来
绘制的框出现的时间太短肉眼无法看出，表现出框一闪而过不够持续，根本看不到测不出来

我的一些解决思路：
1.回溯帧检测:每检测一帧向前回溯6～8帧重新检测
2.由于视频里的目标是运动的，相对于静态图片有其复杂性，采用目标跟踪算法
3.借用视频的时序信息、相邻帧之间的关联性来处理
  因为对于一个井盖（比如出现了20帧），一次也检测不到的概率比较低，我获得了离散轨迹，做插值:
    if detected:
        get(x,y,帧)->队列
        按照时间帧和坐标这两个元素聚类的原则取出点，认为它们都是一个井盖，给予同一个classify
        分批次处理，做插值并平滑处理曲线
        根据每个井盖的（x,y）队列，用同一个大小逐帧绘制框图

4.将此视频分割更短的视频，分多线程反复检测后拼在一起